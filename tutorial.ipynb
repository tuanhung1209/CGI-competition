{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the basic tutorial found in the course on how to sumbit the code\n",
    "it go though all the basic code (data, model, sumbit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QShif6ZLCnmC"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phmNjzQa12-E"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "sample_sub = pd.read_csv(\"input/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSh7uV_4CnmF"
   },
   "source": [
    "Before conducting a full-scale analysis, we will first review a brief overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmJzxS2nCnmG"
   },
   "outputs": [],
   "source": [
    "# Check train data\n",
    "print(f\"train shape: {train.shape}\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0j6hSZwjCnmG"
   },
   "outputs": [],
   "source": [
    "# Check test data\n",
    "print(f\"test shape: {test.shape}\")\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd5aPgQc2Voe"
   },
   "source": [
    "Excluding the `TARGET` column in train data and the `SK_ID_CURR` which represents ID number, you can see that there are 32 types of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NLboDmT3Wsg"
   },
   "source": [
    "### 2.2 Selecting Features\n",
    "\n",
    "It is often difficult to perform data analysis and preprocessing on all features from the beginning. Instead, an easier way to get started is to start with a small number of features and then add features one by one.\n",
    "\n",
    "This notebook will focus on 5 features. For the remaining 25 types of features, please refer to the lecture materials, the methods introduced in this notebook, etc., and perform the analysis on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCF0BZ2lCnmH"
   },
   "outputs": [],
   "source": [
    "use_features = [\n",
    "    \"NAME_CONTRACT_TYPE\",\n",
    "    \"AMT_INCOME_TOTAL\",\n",
    "    \"EXT_SOURCE_2\",\n",
    "    \"OWN_CAR_AGE\",\n",
    "    \"ORGANIZATION_TYPE\",\n",
    "]\n",
    "target = train[\"TARGET\"].values\n",
    "\n",
    "train = train[use_features]\n",
    "train[\"TARGET\"] = target\n",
    "test = test[use_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zCvRUOCCnmH"
   },
   "source": [
    "Let's check the data once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4IL84J-CnmI"
   },
   "outputs": [],
   "source": [
    "# Check train data\n",
    "print(f\"train shape: {train.shape}\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns__iaZ3CnmI"
   },
   "outputs": [],
   "source": [
    "# Check test data\n",
    "print(f\"test shape: {test.shape}\")\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NuP1zcmCnmF"
   },
   "source": [
    "## 3.Visualizing and Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ooo0LA-2CnmI"
   },
   "source": [
    "The first thing we need to do before building the machine learning model is to **understand the data**. We do this by visualizing and analyzing, to deepen our understanding of data distribution, missing values, outliers, correlations, and etc. The results of the analysis obtained at this stage will be useful for preprocessing, feature creation, and selection of machine learning models, which are all important to building models with better prediction ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tnoSfzvCnmJ"
   },
   "source": [
    "### 3.1 Checking missing values\n",
    "In this section, we check for missing values.\n",
    "This is important as **most machine learning models cannot be trained on data with missing values**. If there are missing values, they need to be filled with some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10WA5hRBCnmJ"
   },
   "outputs": [],
   "source": [
    "# Check missing values of train data\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y7FVExVCnmJ"
   },
   "outputs": [],
   "source": [
    "# Check missing values of test data\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erj6eK80CnmJ"
   },
   "source": [
    "We found that there are missing values in `EXT_SOURCE_2` and `OWN_CAR_AGE`. We will deal with these missing values later. Of course, there is a possibility that there are missing values for other features that we are not covering here, so please check them by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrCvOnzO97pC"
   },
   "source": [
    "**Findings**:<br>\n",
    "* Need to deal with missing values in `EXT_SOURCE_2` and `OWN_CAR_AGE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHtdHORMCnmJ"
   },
   "source": [
    "### 3.2 Visualization and analysis of each feature\n",
    "In this section, we visualize each feature and analyze to see what kind of characteristics it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uYeP32t7ZV5"
   },
   "source": [
    "#### TARGET column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYRYwJ_LCnmK"
   },
   "outputs": [],
   "source": [
    "# The distribution of the target (default or not)\n",
    "sns.countplot(data=train, x=\"TARGET\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw4sq8rSCnmK"
   },
   "source": [
    "We can see that the **distribution of the objective variable is highly skewed**. Data in which the distribution of the objective variable is highly skewed in this way is called **unbalanced data**.\n",
    "\n",
    "When dealing with unbalanced data, we need to be particularly careful in selecting evaluation metrics. For example, if you choose accuracy, you will find that simply predicting all zeros will result in a high accuracy. **Choosing such an inappropriate metric can cause the machine learning model to fail to predict well on new data**.\n",
    "\n",
    "Another approach to dealing with unbalanced data is to try to balance the distribution of the target variable. The method of reducing the data of the larger target variable is called undersampling, while the method of increasing the data of the smaller objective variable is called oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V9YM-Y5-M-r"
   },
   "source": [
    "**Findings**:<br>\n",
    "* (May) need to think about methods to mitigate the skewedness of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9DM6eqV77mb"
   },
   "source": [
    "#### NAME_CONTRACT_TYPE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wz_F_PbCnmK"
   },
   "outputs": [],
   "source": [
    "# The distribution of NAME_CONTRACT_TYPE\n",
    "sns.countplot(data=train, x=\"NAME_CONTRACT_TYPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CLL0KhFCnmL"
   },
   "source": [
    "There are two variables in `NAME_CONTRACT_TYPE`, Cash loans and Revolving loans, but they are not evenly distributed. Also, since the machine learning model can only handle data of numeric type, it is necessary to convert the data from string type to numeric type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odEI2M7t-cJ7"
   },
   "source": [
    "**Findings**:<br>\n",
    "* (May) need to think about methods to mitigate the skewedness of the target variable\n",
    "* Need to convert the data from string type to numeric type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twIf6AxD8P5M"
   },
   "source": [
    "#### ORGANIZATION_TYPE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5sFv7BPCnmL"
   },
   "outputs": [],
   "source": [
    "# The distribution of ORGANIZATION_TYPE\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.countplot(data=train, x=\"ORGANIZATION_TYPE\")\n",
    "plt.tick_params(axis=\"x\", rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt51YzxOCnmL"
   },
   "source": [
    "There are many different `ORGANIZATION_TYPE`s, and you can also see that there is an ununiformity in the number of data. This is also a string type feature, so it needs to be converted to a numeric type. Also, the second variable from the left is \"XNA\", which we can infer from its name to be a missing value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nF95Itt-jwW"
   },
   "source": [
    "**Findings**:<br>\n",
    "* Treat \"XNA\" as missing values\n",
    "* Need to convert the data from string type to numeric type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Or9DAj8YLa"
   },
   "source": [
    "#### EXT_SOURCE_2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVlLRi4TCnmL"
   },
   "outputs": [],
   "source": [
    "# The distribution of EXT_SOURCE_2\n",
    "sns.displot(data=train, x=\"EXT_SOURCE_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaEXl1zICnmM"
   },
   "source": [
    "We can see that EXT_SOURCE_2 is normalized between 0 and 1. It seems we can handle this feature as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR91l5IQ-0xX"
   },
   "source": [
    "**Findings**:<br>\n",
    "* No additional preprocessing is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHQYht0C8aYB"
   },
   "source": [
    "#### AMT_INCOME_TOTAL column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WwsHs0JCnmM"
   },
   "outputs": [],
   "source": [
    "# The distribution of AMT_INCOME_TOTAL\n",
    "sns.displot(data=train, x=\"AMT_INCOME_TOTAL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQbzv0D8CnmM"
   },
   "source": [
    "The visualization of `AMT_INCOME_TOTAL` is hard to interpret. THis may be caused by the presence of a small number of outliers that take large values. To visualize data like this, a logarithmic transformation can be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgeKevDMCnmM"
   },
   "outputs": [],
   "source": [
    "# The distribution of AMT_INCOME_TOTAL（Logarithmic transformation）\n",
    "sns.displot(data=train, x=\"AMT_INCOME_TOTAL\", log_scale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4r3xGRMCnmM"
   },
   "source": [
    "We displayed the graph successfully by using logarithmic transformation.\n",
    "The income is supposed to be a continuous value, but it looks like a discrete value. Let's have a look at the type of `AMT_INCOME_TOTAL` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWDiKd98CnmM"
   },
   "outputs": [],
   "source": [
    "# Check the type of AMT_INCOME_TOTAL values\n",
    "len(train[\"AMT_INCOME_TOTAL\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VcQ-MEBCnmN"
   },
   "source": [
    "There are 171202 data in train, but `AMT_INCOME_TOTAL` consists of only 1641 different values. Let's check the top 10 values specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8mETFkCCnmN"
   },
   "outputs": [],
   "source": [
    "# Top 10 values of AMT_INCOME_TOTAL\n",
    "train[\"AMT_INCOME_TOTAL\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82L-z5gQCnmN"
   },
   "source": [
    "It appears that `AMT_INCOME_TOTAL` is not an exact annual income, but rather data compiled from a rounded number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmcAylwL_Jc_"
   },
   "source": [
    "**Findings**:<br>\n",
    "* Should the outlier in the data be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_IUFERV8wDW"
   },
   "source": [
    "#### OWN_CAR_AGE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXBlabmLCnmN"
   },
   "outputs": [],
   "source": [
    "# The distribution of OWN_CAR_AGE\n",
    "sns.displot(data=train, x=\"OWN_CAR_AGE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LheIcW5LCnmN"
   },
   "source": [
    "`OWN_CAR_AGE` can be inferred to be in years from the scale of values. In addition, the distribution is natural from 0 to 40, but there is an unnatural distribution around 60 to 70. It is hard to imagine that the number of years a car has been purchased increases suddenly like this, so they are considered to be outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6aOlsbS_QeD"
   },
   "source": [
    "**Findings**:<br>\n",
    "* Treat numbers above 60 as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl23q2zgCnmN"
   },
   "source": [
    "Up to this point, we have visualized and analyzed each feature. I believe that you have realized that visualization requires some ingenuity and that visualization can deepen your understanding of data. I am sure that the visualization and analysis of the 25 features not covered here will lead to improved forecasting accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzaMVo9x8_cS"
   },
   "source": [
    "### **[Next Steps]**\n",
    "> + Check for missing values for the features you have added in Section 2.2.\n",
    "> + Visualize the features you have added. Is the feature categorical or continuous? What type of graph is most effective to understand it?\n",
    "> + What do you notice about the features? What kind of preprocessing is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsPYkguwCnmO"
   },
   "source": [
    "## 4.Preprocessing and Feature Creation\n",
    "Here, we will conduct the preprocessing and create new features based on what we have learned in the preceding visualization and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL54Ws3zCnmO"
   },
   "source": [
    "### NAME_CONTRACT_TYPE column\n",
    "Convert `NAME_CONTRACT_TYPE` to a numeric type. In this case, “Cash loans” is converted to 0 and “Revolving loans” to 1. This method of simply replacing an integer is called **Label Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C_JEPadCnmO"
   },
   "outputs": [],
   "source": [
    "# Numerization of NAME_CONTRACT_TYPE（Label Encoding）\n",
    "train[\"NAME_CONTRACT_TYPE\"].replace({'Cash loans': 0, 'Revolving loans': 1}, inplace=True)\n",
    "test[\"NAME_CONTRACT_TYPE\"].replace({'Cash loans': 0, 'Revolving loans': 1}, inplace=True)\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1OpbA-yCnmO"
   },
   "source": [
    "### ORGANIZATION_TYPE column\n",
    "Convert `ORGANIZATION_TYPE` to a numeric type. This time, we will convert the variable to numeric in terms of the number of data in the variable. For example, if the number of data in “Police” is 1279 and the number of data in “Bank” is 1385, convert “Police” to 1279 and “Bank” to 1385. This method of replacing the number of data with the number of data is called **Count Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLJPuILuCnmO"
   },
   "outputs": [],
   "source": [
    "# Numerization of ORGANIZATION_TYPE (Count Encoding）\n",
    "organization_ce = train[\"ORGANIZATION_TYPE\"].value_counts()\n",
    "train[\"ORGANIZATION_TYPE\"] = train[\"ORGANIZATION_TYPE\"].map(organization_ce)\n",
    "test[\"ORGANIZATION_TYPE\"] = test[\"ORGANIZATION_TYPE\"].map(organization_ce)\n",
    "\n",
    "train.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0cPsOyMCnmO"
   },
   "source": [
    "### EXT_SOURCE_2 column\n",
    "Fill missing values in `EXT_SOURCE_2`. There are various methods for completing missing values, but in this case, since the number of missing values is small, we simply use the average value to complete the missing values.\n",
    "\n",
    "**IMPORANT**:\n",
    "When you fill the missing values in the test data, you need to **fill with the average of the train data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO-iJnwLCnmO"
   },
   "outputs": [],
   "source": [
    "# Complete missing values of EXT_SOURCE_2 with the average\n",
    "train[\"EXT_SOURCE_2\"].fillna(train[\"EXT_SOURCE_2\"].mean(), inplace=True)\n",
    "test[\"EXT_SOURCE_2\"].fillna(train[\"EXT_SOURCE_2\"].mean(), inplace=True) # Use average of train data to fill test data\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtdYByRsCnmP"
   },
   "source": [
    "### OWN_CAR_AGE column\n",
    "First, we will replace the unnatural outliers that are over 60 as `np.nan` (missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-ojzBAtCnmP"
   },
   "outputs": [],
   "source": [
    "# Treat values above 60 (outliers) in OWN_CAR_AGE as missing values\n",
    "train.loc[train[\"OWN_CAR_AGE\"] >= 60, \"OWN_CAR_AGE\"] = np.nan\n",
    "test.loc[test[\"OWN_CAR_AGE\"] >= 60, \"OWN_CAR_AGE\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUbZ-qsiCnmP"
   },
   "source": [
    "Next, we consider the handling of missing values. The original `OWN_CAR_AGE` had 112992 missing values out of 171202 data. With such a large number of missing values, it is difficult and impractical to properly fill the missing values with some value. Therefore, we will group `OWN_CAR_AGE` by decade (e.g. Group 1: 0-9 years, Group 2: 10-19 years, etc.), then apply **One Hot Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bHTyLv_CnmP"
   },
   "outputs": [],
   "source": [
    "# Divide OWN_CAR_AGE into groups\n",
    "train[\"OWN_CAR_AGE\"] = train[\"OWN_CAR_AGE\"] // 10\n",
    "test[\"OWN_CAR_AGE\"] = test[\"OWN_CAR_AGE\"] // 10\n",
    "\n",
    "train[\"OWN_CAR_AGE\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIcCvBHvCnmP"
   },
   "outputs": [],
   "source": [
    "# Apply One Hot Encoding to OWN_CAR_AGE\n",
    "train_car_age_ohe = pd.get_dummies(train[\"OWN_CAR_AGE\"]).add_prefix(\"OWN_CAR_AGE_\")\n",
    "test_car_age_ohe = pd.get_dummies(test[\"OWN_CAR_AGE\"]).add_prefix(\"OWN_CAR_AGE_\")\n",
    "\n",
    "# Add the one hot encoded columns to train/test\n",
    "train = pd.concat([train, train_car_age_ohe], axis=1)\n",
    "test = pd.concat([test, test_car_age_ohe], axis=1)\n",
    "\n",
    "# Remove original OWN_CAR_AGE\n",
    "train.drop('OWN_CAR_AGE', axis=1, inplace=True)\n",
    "test.drop('OWN_CAR_AGE', axis=1, inplace=True)\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxZerNEvBleb"
   },
   "source": [
    "### **[Next Steps]**\n",
    "> + Apply preprocessing to the features you added. Is it correctly preprocessed?\n",
    "> + Explore other preprocessing methods to apply to the features.\n",
    "> + If you have errors, try reloading the dataset by going back to [Section 2.1](#scrollTo=2TTHzi1c3a5E&line=5&uniqifier=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoKdK60PCnmP"
   },
   "source": [
    "## 5.Building the Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LLoN-aQIsrk"
   },
   "source": [
    "### 5.1 Import Additional Libraries\n",
    "First, we import the necessary libraries for training and evaluation.\n",
    "\n",
    "- `train_test_split`: Split data into training and evaluation data.\n",
    "- `StandardScaler`: Standardize the data.\n",
    "- `roc_auc_score`: Calculate ROC-AUC, the evaluation metric for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVwkQ5b8CnmP"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcgHtFrRJpev"
   },
   "source": [
    "### 5.2 Preparing the Data\n",
    "Split the data into explanatory and target variables. The target variable for this dataset is `TARGET` column and the rest are explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joie-z89KBdg"
   },
   "outputs": [],
   "source": [
    "# Split the data into explanatory and target variables\n",
    "X = train.drop(\"TARGET\", axis=1).values\n",
    "y = train[\"TARGET\"].values\n",
    "X_test = test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M85-Y6M2KQBX"
   },
   "source": [
    "Standardize the data. Standardization is the operation of transforming the values so that the mean is 0 and the variance is 1. Some models, such as logistic regression and neural networks, do not learn well without scaling the values in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8lmkBJbCnmQ"
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X_std = sc.transform(X)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3faa7zQnKmSS"
   },
   "source": [
    "### 5.3 Training the Model\n",
    "We first split the training data into training data and validation data. This method of keeping a portion of the training data for evaluation and not using it for training is called the **holdout method**. This is one method to approximate the model's predictive ability on unknown data (**generalization** performance).\n",
    "\n",
    "Here, we will use 70% of the data as training data and 30% as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PipkeXdKlvK"
   },
   "outputs": [],
   "source": [
    "# Split the original data into the training data and the validation data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_std, y, test_size=0.3, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I62WMgQiMU8J"
   },
   "source": [
    "Now, let's create models with logistic regression and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpzK_zyWCnmQ"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_train_pred = lr.predict_proba(X_train)[:, 1]\n",
    "lr_valid_pred = lr.predict_proba(X_valid)[:, 1]\n",
    "print(f\"Train Score: {roc_auc_score(y_train, lr_train_pred)}\")\n",
    "print(f\"Valid Score: {roc_auc_score(y_valid, lr_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvRvrmC2CnmQ"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_train_pred = rf.predict_proba(X_train)[:, 1]\n",
    "rf_valid_pred = rf.predict_proba(X_valid)[:, 1]\n",
    "print(f\"Train Score: {roc_auc_score(y_train, rf_train_pred)}\")\n",
    "print(f\"Valid Score: {roc_auc_score(y_valid, rf_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg_HBfWDFrdM"
   },
   "source": [
    "We find that random forest results with higher validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSthGRA-ZXaY"
   },
   "source": [
    "### 5.4 (Optional) Ensemble Learning\n",
    "Now that we have created two models, we can try combining these two models for better predictive ability (**ensemble learning**). There are various methods for ensemble learning, but here we will simply take the average of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwEBg6HbGPGn"
   },
   "outputs": [],
   "source": [
    "train_pred = (lr_train_pred + rf_train_pred) / 2\n",
    "valid_pred = (lr_valid_pred + rf_valid_pred) / 2\n",
    "\n",
    "print(f\"Train Score: {roc_auc_score(y_train, train_pred)}\")\n",
    "print(f\"Valid Score: {roc_auc_score(y_valid, valid_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFXE351Dah5p"
   },
   "source": [
    "We find that in this case, ensemble leaning does not contribute to improved score. So, **we will use the random forest model as the final model to make predictions on the test data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R33--AYQGV--"
   },
   "source": [
    "### **[Next Steps]**\n",
    "> + Is holdout method the best method to evaluate your model?\n",
    "> + Is the model's hyperparameters optimized? What hyperparameters needs tuning?\n",
    "> + Explore other models to use to make predictions.\n",
    "> + Explore other ensembling methods to further improve the model's performance.\n",
    "> + If you have errors, try reloading the dataset by going back to [Section 2.1](#scrollTo=2TTHzi1c3a5E&line=5&uniqifier=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn_kdvWYCnmQ"
   },
   "source": [
    "## 6.Creating Prediction Results\n",
    "Finally, let's make a prediction for the test data, and prepare a CSV file to submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVfIg3QgP_8d"
   },
   "source": [
    "### 6.1 Predicting on the test data\n",
    "We found in Sections 5.3 and 5.4 that the best model was random forest model. Therefore, we will use this model to make the final prediction.\n",
    "\n",
    "If you made any changes and found a better model, you will need to change the code below accordingly.\n",
    "\n",
    "```python\n",
    "# If logistic regression model was better\n",
    "pred = lr.predict_proba(X_test_std)[:, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IPNn-_ZCnmQ"
   },
   "outputs": [],
   "source": [
    "# Make predictions for the test data\n",
    "# Change model name if needed\n",
    "pred = rf.predict_proba(X_test_std)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THpn4GVgQD5d"
   },
   "source": [
    "### 6.2 Saving the prediction as CSV file [DO NOT CHANGE]\n",
    "**WARNING**: DO **NOT** CHANGE THE CODES BELOW!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c13Ycte5W047"
   },
   "outputs": [],
   "source": [
    "# Put the prediction into the format of submission\n",
    "sample_sub['TARGET'] = pred\n",
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2PJ33yqnWei"
   },
   "outputs": [],
   "source": [
    "# Create the \"output\" directory if it doesn't exist\n",
    "output_dir = Path.cwd() / \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify the new output file path\n",
    "output_file = output_dir / \"submission.csv\"\n",
    "\n",
    "# Save the CSV file to the \"output\" directory\n",
    "sample_sub.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEIQ6tJFYSs4"
   },
   "source": [
    "That's all for the tutorial of Home Credit Default Risk competition! Submit your CSV file to Omnicampus to see the result.\n",
    "\n",
    "Only 5 out of 30 features are covered in this notebook, so there are a lot of room for improvement. Check out **[Next Steps]** in each section to see what you can do to improve your score."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
