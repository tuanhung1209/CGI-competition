{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdaeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82116d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879025c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a17b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run features_testing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into explanatory and target variables\n",
    "X = train.drop(\"TARGET\", axis=1).values\n",
    "y = train[\"TARGET\"].values\n",
    "X_test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X[0]))\n",
    "print(len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import PowerTransformer\n",
    "#\n",
    "#pt = PowerTransformer(method='yeo-johnson')\n",
    "#X_pt = pt.fit_transform(X)\n",
    "#X_test_pt = pt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "\n",
    "X_std = sc.transform(X)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f767c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original data into the training data and the validation data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_std, y, test_size=0.3, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),  # Reduced from 256\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),  # Reduced from -1 to 12\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),  # Narrowed range\n",
    "\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.7, 1.0),  # Narrowed\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.7, 1.0),  # Narrowed\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 5),  # Reduced from 10\n",
    "\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),  # Narrowed\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-4, 1.0, log=True),  # Narrowed\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-4, 1.0, log=True),  # Narrowed\n",
    "\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),  # Reduced from 3000\n",
    "        \"verbose\": -1,\n",
    "        \n",
    "        # GPU parameters\n",
    "        \"device\": \"gpu\" ,\n",
    "        \"gpu_platform_id\": 0 ,\n",
    "        \"gpu_device_id\": 0 ,\n",
    "    }\n",
    "\n",
    "    # Initialize k-fold cross-validation\n",
    "    # Reduced to 3 folds for faster training\n",
    "    n_splits = 3\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    auc_scores = []\n",
    "    \n",
    "    # Convert X_std to DataFrame to avoid feature names warning\n",
    "    # Create feature names if they don't exist\n",
    "    if isinstance(X_std, np.ndarray):\n",
    "        feature_names = [f'feature_{i}' for i in range(X_std.shape[1])]\n",
    "        x_train_df = pd.DataFrame(X_std, columns=feature_names)\n",
    "    else:\n",
    "        x_train_df = X_std\n",
    "    \n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(x_train_df, y)):\n",
    "        x_train_fold = x_train_df.iloc[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        x_val_fold = x_train_df.iloc[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        \n",
    "        model.fit(\n",
    "            x_train_fold,\n",
    "            y_train_fold,\n",
    "            eval_set=[(x_val_fold, y_val_fold)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=30),  # Reduced from 100\n",
    "                lgb.log_evaluation(period=0)  # Silent training\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict_proba(x_val_fold)[:, 1]\n",
    "        fold_auc = roc_auc_score(y_val_fold, preds)\n",
    "        auc_scores.append(fold_auc)\n",
    "        \n",
    "        print(f\"Trial {trial.number} - Fold {fold + 1}/{n_splits} AUC: {fold_auc:.6f}\")\n",
    "    \n",
    "    # Return mean AUC across all folds\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    print(f\"Trial {trial.number} - Mean AUC: {mean_auc:.6f} (Â±{std_auc:.6f})\\n\")\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "# Create and run the study\n",
    "# n_jobs=1 when using GPU (GPU doesn't benefit from parallel trials)\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"lgbm_optimization_kfold\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),  # For reproducibility\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)  # Prune bad trials early\n",
    ")\n",
    "\n",
    "n_trials = 20 \n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=n_trials,\n",
    "    n_jobs=1,  # LightGBM GPU doesn't support parallel training\n",
    "    show_progress_bar=True,\n",
    "    timeout=7200  # 2 hour timeout as safety measure\n",
    ")\n",
    "\n",
    "print(\"\\nBest Mean AUC:\", study.best_value)\n",
    "print(\"Best hyperparameters:\\n\", study.best_params)\n",
    "\n",
    "# Train final model with best parameters on full training data\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"verbose\": -1,\n",
    "\n",
    "    \"device\": \"gpu\" ,\n",
    "    \"gpu_platform_id\": 0 ,\n",
    "    \"gpu_device_id\": 0 ,\n",
    "})\n",
    "\n",
    "# Split data for final validation\n",
    "X_train_final, X_valid_final, y_train_final, y_valid_final = train_test_split(\n",
    "    X_std, y, test_size=0.3, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(**best_params)\n",
    "lgbm.fit(X_train_final, y_train_final, eval_set=[(X_valid_final, y_valid_final)])\n",
    "\n",
    "lgbm_train_pred = lgbm.predict_proba(X_train_final)[:, 1]\n",
    "lgbm_valid_pred = lgbm.predict_proba(X_valid_final)[:, 1]\n",
    "\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"Train Score: {roc_auc_score(y_train_final, lgbm_train_pred)}\")\n",
    "print(f\"Valid Score: {roc_auc_score(y_valid_final, lgbm_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c05234",
   "metadata": {},
   "source": [
    "### under this is the output part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the test data\n",
    "# Change model name if needed\n",
    "pred = lgbm.predict_proba(X_test_std)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59160cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the prediction into the format of submission\n",
    "sample_sub['TARGET'] = pred\n",
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"output\" directory if it doesn't exist\n",
    "output_dir = Path.cwd() / \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify the new output file path\n",
    "output_file = output_dir / \"submission.csv\"\n",
    "\n",
    "# Save the CSV file to the \"output\" directory\n",
    "sample_sub.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
